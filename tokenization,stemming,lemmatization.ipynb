{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b11b9d-7527-40ff-ad7e-e16e3fa0dc35",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "881cc618-560e-438d-bc3a-1c3d40420b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"Are you curious about tokenization ? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2de4972c-8a27-40cd-a391-4dcbe05ef1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d62f98d-c319-4f62-8003-bc5384c11261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saite\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82698a4b-9cdd-4c18-ae62-71febccb1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize_list=sent_tokenize(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afaca451-8f46-4d65-bc7c-c36ac2487153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are you curious about tokenization ?', \"Let's see how it works!\", 'We need to analyze a couple of sentences with punctuations to see it in action.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28516016-5482-4b29-a151-9db0f0bb0738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokenizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff270d6d-ead8-4f4e-b57a-05dec49e6035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'s\", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokenize_list=word_tokenize(text1)\n",
    "print(word_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa444ecc-e7c7-4288-b1ad-2bb0f48c74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e6cf6f6-17dd-491c-8fc2-2285d03d3702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'\", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "punctword=WordPunctTokenizer()\n",
    "print(punctword.tokenize(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5fdac4-8009-41f6-a007-be498da57e4d",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90f78b35-877d-41fa-8eac-8a93c32cf585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "779980f4-4d93-4617-a030-b7875454618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating few words\n",
    "Words=['table','probably','wolves','playing','is','dog','the','beaches','grounded','dreamt','envision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37824392-51d1-4b90-a17f-546f897190ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemmers=['POTTER','LANCASTER','SNOWBALL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f8b7946-e4b5-408e-9fab-77ab389244b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemmer_potter=PorterStemmer()\n",
    "Stemmer_lancaster=LancasterStemmer()\n",
    "Stemmer_snowball=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ed168e5-af56-4d3e-b02f-9a8926150122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             WORD          POTTER       LANCASTER        SNOWBALL \n",
      "\n"
     ]
    }
   ],
   "source": [
    "formated_row='{:>16}'*(len(Stemmers)+1)\n",
    "print('\\n',formated_row.format('WORD',*Stemmers),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9e4a8d6-d3ac-43ea-bd50-8abec4f33265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           table            tabl            tabl            tabl\n",
      "        probably         probabl            prob         probabl\n",
      "          wolves            wolv            wolv            wolv\n",
      "         playing            play            play            play\n",
      "              is              is              is              is\n",
      "             dog             dog             dog             dog\n",
      "             the             the             the             the\n",
      "         beaches           beach           beach           beach\n",
      "        grounded          ground          ground          ground\n",
      "          dreamt          dreamt          dreamt          dreamt\n",
      "        envision           envis           envid           envis\n"
     ]
    }
   ],
   "source": [
    "for word in Words: #it will apply the stemming to the every word present in the words\n",
    "    Stemmed_word=[Stemmer_potter.stem(word),\n",
    "                  Stemmer_lancaster.stem(word),\n",
    "                  Stemmer_snowball.stem(word)]\n",
    "    print(formated_row.format(word,*Stemmed_word)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2be97-437b-4fd6-a15a-883a8d3b6f22",
   "metadata": {},
   "source": [
    "# LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c207ca07-272b-42ed-8ae6-cecf67c3fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3830714b-aa64-454d-961a-522ffd0595ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Words=['table','probably','wolves','playing','is','dog','the','beaches','grounded','dreamt','envision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59d30eec-40ce-4429-862f-1f91663be6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizers=['NOUN LEMMATIZER','VERB LEMMATIZER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d7055f3-adba-4860-af8d-72c971ad0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer_wordnet=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df45bf9e-0f2d-427a-858e-3e0beefedf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_row='{:>24}'*(len(lemmatizers)+1)\n",
    "print('\\n',formatted_row.format('WORD',*lemmatizers),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "185808d2-7114-403d-b1f4-3b50d1f8e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   table                   table                   table\n",
      "                probably                probably                probably\n",
      "                  wolves                    wolf                  wolves\n",
      "                 playing                 playing                    play\n",
      "                      is                      is                      be\n",
      "                     dog                     dog                     dog\n",
      "                     the                     the                     the\n",
      "                 beaches                   beach                   beach\n",
      "                grounded                grounded                  ground\n",
      "                  dreamt                  dreamt                   dream\n",
      "                envision                envision                envision\n"
     ]
    }
   ],
   "source": [
    "for word in Words:\n",
    "    lemmatizer_word=[lemmatizer_wordnet.lemmatize(word,pos='n'),\n",
    "                     lemmatizer_wordnet.lemmatize(word,pos='v')]\n",
    "    print(formatted_row.format(word,*lemmatizer_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afca26-c845-4d43-baf9-c70e63e527b2",
   "metadata": {},
   "source": [
    "# Dividing the text using chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de081b90-300f-4589-a754-53886d0dcd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "da13e725-51c7-47d6-ba49-ea078d6112fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(data,num_words):\n",
    "        words=data.split(' ')\n",
    "        output=[]\n",
    "        cur_count=0\n",
    "        cur_words=[]\n",
    "        for word in Words:\n",
    "            cur_words.append(word)\n",
    "            cur_count+=1\n",
    "            if cur_count==num_words:\n",
    "                output.append(' '.join(cur_words))\n",
    "                cur_words=[]\n",
    "                cur_count=0\n",
    "                return output\n",
    "            if __name__=='__main__':\n",
    "                data=''.join(brown.words()[:10000])\n",
    "                num_words=1700\n",
    "                chunks=[]\n",
    "                counter=0\n",
    "                text_chunks=splitter(data,num_words)\n",
    "                print(\"number of text chunks =\",len(text_chunks))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
